{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a827758b-c7cc-4c29-9e37-f92bb2a4adc2",
   "metadata": {},
   "source": [
    "# Homework 1 - Data munging and Linear Regression\n",
    "\n",
    "Brandeis University\n",
    "COSI 104-A Intro to Machine Learning\n",
    "Spring 2025\n",
    "Authors: Dylan Cashman and Binyamin Friedman\n",
    "\n",
    "## Part 1: Data Munging (50 points)\n",
    "\n",
    "### Installation\n",
    "By now, you should have installed Jupyter Notebook and be familiar with how to run it with a virtual environment. To automatically install the dependencies listed in `requirements.txt`, you can run the following command in the terminal:\n",
    "    \n",
    "```bash\n",
    "pip install -r requirements.txt\n",
    "```\n",
    "\n",
    "Alternatively, your IDE (PyCharm, or VSCode) may auto-detect the `requirements.txt` file and install the required libraries.\n",
    "\n",
    "### Overview\n",
    "[Pandas](https://pandas.pydata.org/) is a data analysis library for efficiently working with tables of data. To complete this task, you can reference their documentation on [DataFrames](https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.html), which is Pandas' primary data structure. Pandas is built on top of [NumPy](https://numpy.org/), which is a widely-used library that provides super efficient multidimensional arrays. Many of the operations you will perform can either be done with higher-level Pandas operations, or done directly with NumPy. Either way, we want your final output to be a NumPy array, since NumPy ndarrays are the standard for use in scientific computing libraries. You can convert a Pandas DataFrame to a NumPy array with the `to_numpy()` method.\n",
    " \n",
    "[scikit-learn](https://scikit-learn.org/stable/) is a machine learning library that offers many useful algorithm implementations. It has a higher level of abstraction compared to PyTorch and Tensorflow. For this assignment, you should reference their documentation on [data preprocessing](https://scikit-learn.org/stable/modules/preprocessing.html) and [comparing matrices](https://docs.scipy.org/doc/scipy/reference/spatial.distance.html).\n",
    "\n",
    "Note that the final line of a cell will be automatically displayed if it is not assigned to a variable. Your solution to a problem can be the last line of the cell, OR it can be assigned to a descriptive variable name.\n",
    "\n",
    "Modify the skeleton code at your own risk."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbdffd9053afecd0",
   "metadata": {},
   "source": [
    "#### Loading data\n",
    "Jupyter notebooks are stateful, which means variables and imports persist between cells. After running the next cell, you will have access to the local variables `df` and `matrix` in all subsequent cells. "
   ]
  },
  {
   "cell_type": "code",
   "id": "57c01ec2",
   "metadata": {},
   "source": [
    "# These aliases are standard practice for pandas and numpy\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# This loads the data into a Pandas DataFrame\n",
    "df = pd.read_csv('sample_data.csv')\n",
    "\n",
    "# We can always convert a dataframe to a numpy matrix by calling the `to_numpy` method\n",
    "matrix = df.to_numpy()\n",
    "\n",
    "# Display the first few rows of the dataframe\n",
    "df.head()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "ba648c9e6900e4d8",
   "metadata": {},
   "source": [
    "#### a) Randomly select 10 rows from the matrix without replacement to create a subset (10 points)\n",
    "\n",
    "Remember, many basic operations can be accomplished with both Pandas and NumPy. Random selection is a method on `pandas.DataFrame` and also a function in `numpy.random`. We leave you to look at the documentation and decide which one to use. "
   ]
  },
  {
   "cell_type": "code",
   "id": "8b855117",
   "metadata": {},
   "source": [
    "# Solution here\n",
    "\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "bab035043a01a753",
   "metadata": {},
   "source": [
    "#### b) Select all rows from the matrix where the value of the `THHLD_NUMKID` column is greater than 4 (10 points)"
   ]
  },
  {
   "cell_type": "code",
   "id": "6699dd8e",
   "metadata": {},
   "source": [
    "# Solution here\n",
    "\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "31f0a3fb010ed1c7",
   "metadata": {},
   "source": "#### c) Fetch the first sample (row) from the matrix (5 points)"
  },
  {
   "cell_type": "code",
   "id": "b765885a",
   "metadata": {},
   "source": [
    "# Solution here\n",
    "\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "#### d) Apply feature normalization to the matrix using sklearn.StandardScaler, which standardizes each feature to have a mean of 0 and a standard deviation of 1 (5 points)\n",
    "\n",
    "Feature normalization is a common preprocessing step that adjusts the range of each feature to be within a similar scale. You can see that this operation is found in scikit-learn's preprocessing module."
   ],
   "id": "f39b70faf1108b24"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Solution here\n",
    "\n"
   ],
   "id": "5755ba5b",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "f38f64ff80a1a72b",
   "metadata": {},
   "source": [
    "#### e) Calculate the cosine similarity between two samples from the matrix by subtracting the cosine distance from 1 (10 points)\n",
    "\n",
    "[SciPy](https://scikit-learn.org/stable/) is yet another Python library commonly used for scientific computing. SciPy has more fundamental algorithms than scikit-learn (which is focused on machine learning), related to optimization, integration, and linear algebra. Don't be overwhelmed by the number of libraries we're using! Everything is well-documented, and most of the functionality you'll need is a Google search away.\n",
    "\n",
    "Cosine similarity is just a way of measuring the distance between two vectors. In other words, how similar they are."
   ]
  },
  {
   "cell_type": "code",
   "id": "693d06d0",
   "metadata": {},
   "source": [
    "from scipy.spatial.distance import cosine\n",
    "\n",
    "# Select two samples from the matrix with no NaN values\n",
    "# In practice, you need to decide how to handle NaN values, not just ignore them\n",
    "sample1 = matrix[0]\n",
    "sample2 = matrix[2]\n",
    "\n",
    "# Solution here\n",
    "\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "ca19a36b2ca82be4",
   "metadata": {},
   "source": [
    "#### f) Calculate the product of two matrices (10 points)\n",
    "\n",
    "Take a look at the NumPy documentation for how to perform matrix multiplication. And feel free to do it by hand if you want to double-check your answer!"
   ]
  },
  {
   "cell_type": "code",
   "id": "1c99e71a",
   "metadata": {},
   "source": [
    "import numpy as np\n",
    "\n",
    "A_a = np.array([[1, 2], [4, 5], [7, 8]])\n",
    "B_a = np.array([[1, 1, 0], [0, 1, 1]])\n",
    "\n",
    "# Solution here\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "d58a687f-b8d2-433e-892a-9158bb986830",
   "metadata": {},
   "source": [
    "## Part 2: Linear Regression (50 points)\n",
    "\n",
    "In this part, you will use the training dataset (training_data.csv) to build a linear regression model to estimate `avgAnnCount`, `avgDeathsPerYear`, `TARGET_deathRate`, and `incidenceRate`. Then you will apply your model to the test dataset (test_data.csv) and analyze the results.\n",
    "\n",
    "Please read the comments carefully and write your code in the designated places.\n",
    "\n",
    "#### a) Creating a Model (15 points)\n",
    "Create a linear regression model. We have supplied some important code to help you get started.\n",
    "1. We load the raw data from a CSV file.\n",
    "2. The preprocessing pipeline is partially defined for you. Your task is to complete the `handle_non_numeric` method. You might want to add additional preprocesing, like through scaling or normalization, if you think it helps you build a better model.\n",
    "3. You should use the **normal equation** to learn the correct model parameters (you may not use sklearn.linear_model.LinearRegression, follow the procedure in the textbook instead). \n",
    "This should all result in a final variable `theta` that contains the fitted model parameters."
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "raw_data.head()",
   "id": "e862b31a0676e7a8",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.preprocessing import FunctionTransformer\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "targets = ['avgAnnCount', 'avgDeathsPerYear', 'TARGET_deathRate', 'incidenceRate']\n",
    "\n",
    "# csv files are easy to work with!\n",
    "# Note that Python also comes with a csv module out of the box\n",
    "raw_data = pd.read_csv('training_data.csv')\n",
    "\n",
    "def handle_non_numeric(data):\n",
    "    \"\"\"\n",
    "    Student's solution here. \n",
    "    \n",
    "    You can drop any non-numeric data, and/or attempt to extract numerical information from it (for example, `binnedInc` is a string containing the median income of the top and bottom decile).\n",
    "    \n",
    "    :param data: data from the pipeline\n",
    "    :return: data with non-numeric fields handled\n",
    "    \"\"\"\n",
    "    return data\n",
    "\n",
    "def add_bias(data):\n",
    "    \"\"\"\n",
    "    This function adds a bias term to the data. The NumPy syntax is confusing, but we\n",
    "    encourage you to look up the documentation and understand what's happening here. \n",
    "    np.ones((data.shape[0], 1)) makes a Mx1 column vector of ones, and np.hstack concatenates it to the data horizontally (whereas np.vstack would concatenate vertically).\n",
    "    \n",
    "    :param data: data from the pipeline\n",
    "    :return: data with a bias term added\n",
    "    \"\"\"\n",
    "    return np.hstack((np.ones((data.shape[0], 1)), data))\n",
    "\n",
    "# Here we preprocess the data:\n",
    "# The sklearn pipeline is a powerful tool for chaining together data transformations. It\n",
    "# chains together Transformers, which each perform some operation on data. The overall \n",
    "# pipeline is also a Transformer. This allows for some great code reusability.\n",
    "# We make use of the FunctionTransformer to add our custom functions to the pipeline.\n",
    "preprocess_pipeline = Pipeline([\n",
    "    ('non_numeric', FunctionTransformer(handle_non_numeric)),  # Handle non-numeric fields\n",
    "    ('imputer', SimpleImputer(strategy='median')),  # Fill in missing values with the median\n",
    "    ('bias', FunctionTransformer(add_bias)),  # Add a bias term\n",
    "])\n",
    "\n",
    "# Separate the data into targets and preprocessed features\n",
    "training_data_y = raw_data[targets].to_numpy()\n",
    "\n",
    "# Note that in one line we drop the targets from the raw_data, and then use the pipeline with the fit_transform method\n",
    "training_data_x = preprocess_pipeline.fit_transform(raw_data.drop(targets, axis=1))\n",
    "\n",
    "# YOUR SOLUTION HERE - Define the normal solution and use it to learn theta\n",
    "# Learn the model parameters\n",
    "# def normal_equation(x, y):\n",
    "#     return x\n",
    "# \n",
    "# theta = normal_equation(training_data_x, training_data_y)\n",
    "theta = None"
   ],
   "id": "73316db2",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "aa011c35d441e7cc",
   "metadata": {},
   "source": [
    "\n",
    "#### b) Model Accuracy (10 points) \n",
    "Calculate the model's mean squared error (using `theta` calculated in the previous part) on both the training set and the test set. Make sure to preprocess the test set as you did on the training set. Which one is higher? Why is this the case?\n",
    "\n",
    "Start by loading the test data from `test_data.csv`."
   ]
  },
  {
   "cell_type": "code",
   "id": "952861d1-0553-41f5-95e9-120ed8fd29ed",
   "metadata": {},
   "source": [
    "# Solution here\n",
    "\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "96ac154327f030e6",
   "metadata": {},
   "source": [
    "*Explanation here*\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c454bbb31cf060af",
   "metadata": {},
   "source": [
    "#### c) Analyzing the Learned Parameters (10 points)\n",
    "For each target attribute, what are the most and least predictive features for our `target`? Explain why these make sense. The definitions of the data attributes should be helpful for theorizing why certain features are predictive. They can be found in `README.md`.\n",
    "\n",
    "HINT: Everything you need to know is in the `theta` variable."
   ]
  },
  {
   "cell_type": "code",
   "id": "d37affb893290c4b",
   "metadata": {},
   "source": "# Solution here\n",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "615087d96b08ad12",
   "metadata": {},
   "source": "*Explanation here*\n"
  },
  {
   "cell_type": "markdown",
   "id": "b14e28d6a05cbad9",
   "metadata": {},
   "source": [
    "#### d) Plotting Data (10 points) \n",
    "Knowing how to visualize data and share your findings is a critical skill. Matplotlib is a widely-used library for plotting scientific data. If you are unfamiliar with it, you can reference their [documentation](https://matplotlib.org/stable/contents.html) or look at some of the many tutorials available online.\n",
    "\n",
    "For this question, you will create two plots,\n",
    " 1. Plot the `medIncome` attribute against the `incidenceRate` attribute in the test data (remember to apply your preprocessing pipeline). On top of this, overlay the linear regression model's best fit line (HINT: The slope and y-intercept are stored in `theta`).\n",
    " 2. Do the same for the `medIncome` attribute against the `avgDeathsPerYear` attribute.\n",
    "\n",
    "What interesting conclusion could you *possibly* draw from these plots?\n",
    "\n",
    "We provide code to generate the scatterplot and best fit line for the first plot.  You must include a title, labels for the x and y axes, and a legend to differentiate the data points from the regression line.  Then, you must generate the second plot on your own.\n"
   ]
  },
  {
   "cell_type": "code",
   "id": "1d4966e48d393b3c",
   "metadata": {},
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "\n",
    "# Solution here\n",
    "def plot_attribute_against_target(attribute, target):\n",
    "    \"\"\"\n",
    "    This function plots the given attribute against the target variable. You must fill it out to overlay the best fit line from the model, and add labels and a title.\n",
    "    \n",
    "    :param attribute: The attribute to plot on the x-axis\n",
    "    :param target: The target variable to plot on the y-axis\n",
    "    \"\"\"\n",
    "    \n",
    "    # We preprocess the test data to get the transformed x values\n",
    "    # This is necessary for overlaying the best fit line, since the model's parameters are learned from preprocessed data\n",
    "    raw_test_data = pd.read_csv('test_data.csv')\n",
    "    test_data_x = preprocess_pipeline.transform(raw_test_data.drop(targets, axis=1))\n",
    "    \n",
    "    # We get the index of the attribute and target in the final frame. This is necessary \n",
    "    # because your handle_non_numeric function will change the indices of some of the \n",
    "    # columns.\n",
    "    final_frame = handle_non_numeric(raw_test_data.drop(targets, axis=1))\n",
    "    attribute_i = final_frame.columns.get_loc(attribute) + 1  # +1 because of the bias term\n",
    "    \n",
    "    x_values = test_data_x[:, attribute_i]\n",
    "    y_values = raw_test_data[target]\n",
    "    \n",
    "    plt.scatter(x_values, y_values, label='Data')\n",
    "    \n",
    "    income_index = final_frame.columns.get_loc(attribute)\n",
    "    target_index = targets.index(target)\n",
    "    best_fit_line = theta[income_index + 1][target_index] * x_values + theta[0][target_index]\n",
    "\n",
    "    plt.plot(x_values, best_fit_line, color='red', label='Regression Line')\n",
    "    \n",
    "    # YOUR CODE: Add a title, labels, and a legend\n",
    "    plt.show()\n",
    "    \n",
    "plot_attribute_against_target('medIncome', 'incidenceRate')\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "6358a2da65549f58",
   "metadata": {},
   "source": [
    "*Explanation here*\n",
    "\n",
    "We see that incidence rate is positively correlated with median income, while average deaths per year is negatively correlated with median income. It's likely that since wealthier areas have access to better healthcare, they have lower death rates but higher diagnosis rates."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ef96a20076aaa15",
   "metadata": {},
   "source": [
    "#### README.md (5 points)\n",
    "In the README cell below, please explain how to run your code, load the model, and interpret the results. The README should provide any necessary information about the code environment and give a broad overview of what your code accomplishes, so that someone unfamiliar with the project could understand its parts."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7baf10ceb90eb351",
   "metadata": {},
   "source": [
    "*README.md*\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
